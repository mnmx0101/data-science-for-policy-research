{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cadd73f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================\n",
    "# Machine Learning: Conventional vs Advanced Models\n",
    "# =========================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, mean_squared_error, r2_score\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression, Ridge, Lasso\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# ------------------------------\n",
    "# Dataset: Titanic for classification\n",
    "# ------------------------------\n",
    "titanic = sns.load_dataset(\"titanic\").dropna(subset=[\"age\", \"fare\", \"sex\", \"class\", \"survived\"])\n",
    "df = titanic.copy()\n",
    "df[\"sex\"] = df[\"sex\"].map({\"male\": 0, \"female\": 1})\n",
    "df[\"class\"] = df[\"class\"].map({\"First\": 1, \"Second\": 2, \"Third\": 3})\n",
    "\n",
    "X = df[[\"age\", \"fare\", \"sex\", \"class\"]]\n",
    "y = df[\"survived\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# ------------------------------\n",
    "# Part 1: Conventional Models\n",
    "# ------------------------------\n",
    "print(\"\\n=== Logistic Regression (Baseline) ===\")\n",
    "logreg = LogisticRegression(max_iter=1000)\n",
    "logreg.fit(X_train, y_train)\n",
    "y_pred_log = logreg.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_log))\n",
    "\n",
    "# ------------------------------\n",
    "# Part 2: Regularization\n",
    "# ------------------------------\n",
    "print(\"\\n=== Ridge Logistic Regression ===\")\n",
    "ridge = LogisticRegression(penalty=\"l2\", C=1.0, solver=\"lbfgs\", max_iter=1000)\n",
    "ridge.fit(X_train, y_train)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, ridge.predict(X_test)))\n",
    "\n",
    "print(\"\\n=== LASSO Logistic Regression ===\")\n",
    "lasso = LogisticRegression(penalty=\"l1\", solver=\"liblinear\", max_iter=1000)\n",
    "lasso.fit(X_train, y_train)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, lasso.predict(X_test)))\n",
    "\n",
    "# ------------------------------\n",
    "# Part 3: Advanced Tree Models\n",
    "# ------------------------------\n",
    "print(\"\\n=== Random Forest ===\")\n",
    "rf = RandomForestClassifier(n_estimators=200, max_depth=5, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, rf.predict(X_test)))\n",
    "\n",
    "print(\"\\n=== Gradient Boosting ===\")\n",
    "gb = GradientBoostingClassifier(n_estimators=200, learning_rate=0.05, max_depth=3, random_state=42)\n",
    "gb.fit(X_train, y_train)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, gb.predict(X_test)))\n",
    "\n",
    "# ------------------------------\n",
    "# Part 4: Compare Models\n",
    "# ------------------------------\n",
    "results = {\n",
    "    \"Logistic\": accuracy_score(y_test, y_pred_log),\n",
    "    \"Ridge\": accuracy_score(y_test, ridge.predict(X_test)),\n",
    "    \"LASSO\": accuracy_score(y_test, lasso.predict(X_test)),\n",
    "    \"Random Forest\": accuracy_score(y_test, rf.predict(X_test)),\n",
    "    \"Gradient Boosting\": accuracy_score(y_test, gb.predict(X_test))\n",
    "}\n",
    "pd.Series(results).plot(kind=\"bar\", figsize=(8,4), title=\"Model Accuracy Comparison\")\n",
    "plt.show()\n",
    "\n",
    "# ------------------------------\n",
    "# Part 5: Unsupervised Learning\n",
    "# ------------------------------\n",
    "print(\"\\n=== K-Means Clustering (unsupervised) ===\")\n",
    "kmeans = KMeans(n_clusters=2, random_state=42)\n",
    "clusters = kmeans.fit_predict(X)\n",
    "\n",
    "df[\"cluster\"] = clusters\n",
    "sns.scatterplot(x=\"age\", y=\"fare\", hue=\"cluster\", data=df, palette=\"Set1\")\n",
    "plt.title(\"K-Means Clustering of Titanic Passengers\")\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n=== PCA (dimensionality reduction) ===\")\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X)\n",
    "plt.scatter(X_pca[:,0], X_pca[:,1], c=y, cmap=\"coolwarm\", alpha=0.7)\n",
    "plt.title(\"PCA Projection Colored by Survival\")\n",
    "plt.xlabel(\"PC1\")\n",
    "plt.ylabel(\"PC2\")\n",
    "plt.show()\n",
    "\n",
    "# ------------------------------\n",
    "# Mission Task\n",
    "# ------------------------------\n",
    "# 1. Add Probit Regression (use statsmodels).\n",
    "# 2. Try Ridge & LASSO regression on mpg dataset (predict 'mpg').\n",
    "# 3. Compare Random Forest vs Gradient Boosting performance on classification.\n",
    "# 4. Experiment with K-Means (different k values).\n",
    "# 5. Visualize PCA components with different datasets.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
