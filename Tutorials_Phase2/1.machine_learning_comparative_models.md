# Module: Machine Learning â€” From Conventional to Advanced Models

---

## 1. Motivation
Machine Learning provides tools to uncover patterns and make predictions.  
But not all algorithms are created equal. Some are **simple and interpretable**, while others are **complex but powerful**.  

In this module, we compare:
- **Conventional algorithms** (Linear, Logistic, Probit).  
- **Advanced conventional** (Ridge, LASSO).  
- **Advanced tree-based models** (Random Forest, Gradient Boosting).  
- **Unsupervised learning** (K-Means, PCA).  

The goal is to **understand trade-offs**:  
- Interpretability vs Predictive Power.  
- Simplicity vs Complexity.

---

## 2. Supervised vs Unsupervised Learning

### Supervised Learning
- Data: \( (X, y) \) with labels.  
- Task: Learn \( f(X) \approx y \).  

**Regression (continuous):**
\[
y \in \mathbb{R}, \quad y = \beta_0 + \sum_{j=1}^p \beta_j x_j + \epsilon
\]

**Classification (categorical):**
\[
P(y = k | X) = f_\theta(X), \quad y \in \{1, \dots, K\}
\]

---

### Unsupervised Learning
- Data: \( X \), no labels.  
- Task: Discover structure or reduce dimensions.  

**Clustering (K-Means):**
\[
\min_{C_1,\dots,C_k} \sum_{i=1}^k \sum_{x \in C_i} \| x - \mu_i \|^2
\]

**Dimensionality Reduction (PCA):**
\[
\max_{w} \frac{w^T \Sigma w}{w^T w}, \quad \text{first eigenvector of covariance matrix}
\]

---

## 3. Algorithms

### 3.1 Conventional (Baseline)
- **Linear Regression** (OLS): minimizes squared error.  
- **Logistic Regression** (Logit):  
\[
P(y=1|x) = \frac{1}{1+e^{-(\beta_0 + \beta^T x)}}
\]  
- **Probit Model**: assumes latent normal distribution.  
\[
P(y=1|x) = \Phi(\beta_0 + \beta^T x)
\]  
where \( \Phi \) is the normal CDF.

---

### 3.2 Advanced Conventional: Regularization
- **Ridge Regression (L2 penalty):**
\[
\min_\beta \|y - X\beta\|^2 + \lambda \|\beta\|_2^2
\]
- **LASSO (L1 penalty):**
\[
\min_\beta \|y - X\beta\|^2 + \lambda \|\beta\|_1
\]

Ridge shrinks coefficients, LASSO can set some to zero (**feature selection**).

---

### 3.3 Advanced: Tree-Based Ensembles
- **Random Forest**: Many decision trees, bagging + feature randomness.  
- **Gradient Boosting**: Sequential trees correcting errors (gradient descent).  

---

## 4. Unsupervised Examples
- **K-Means Clustering**: Grouping passengers into segments.  
- **PCA**: Reducing dimensionality for visualization.  

---

## 5. Objectives
- Compare conventional vs advanced models.  
- See how regularization improves linear models.  
- Show how tree models outperform in complex patterns.  
- Introduce unsupervised methods as exploratory tools.  

---

## 6. Expected Outcomes
You will be able to:
- Fit Linear/Logistic/Probit models.  
- Apply Ridge & LASSO.  
- Train Random Forest and Gradient Boosting models.  
- Run K-Means and PCA.  
- Evaluate and compare models.  

---

## References
- [Scikit-learn Linear Models](https://scikit-learn.org/stable/modules/linear_model.html)  
- [Scikit-learn Ensemble Methods](https://scikit-learn.org/stable/modules/ensemble.html)  
- [Clustering with Scikit-learn](https://scikit-learn.org/stable/modules/clustering.html)  
- [PCA Explained](https://scikit-learn.org/stable/modules/decomposition.html#pca)  
