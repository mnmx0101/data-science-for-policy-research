{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4d4216",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# Data Acquisition: APIs & Web Scraping\n",
    "# ==============================\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# ------------------------------\n",
    "# Part 1: Working with APIs\n",
    "# ------------------------------\n",
    "\n",
    "print(\"=== Example 1: Bitcoin Price API ===\")\n",
    "\n",
    "url = \"https://api.coindesk.com/v1/bpi/currentprice.json\"\n",
    "response = requests.get(url)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    data = response.json()\n",
    "    print(\"Bitcoin Price (USD):\", data[\"bpi\"][\"USD\"][\"rate\"])\n",
    "else:\n",
    "    print(\"Error fetching data:\", response.status_code)\n",
    "\n",
    "# Convert to DataFrame\n",
    "df_api = pd.DataFrame([{\n",
    "    \"time\": data[\"time\"][\"updated\"],\n",
    "    \"price_usd\": data[\"bpi\"][\"USD\"][\"rate_float\"]\n",
    "}])\n",
    "display(df_api)\n",
    "\n",
    "\n",
    "print(\"\\n=== Example 2: NASA Astronomy Picture of the Day (APOD) API ===\")\n",
    "\n",
    "nasa_url = \"https://api.nasa.gov/planetary/apod?api_key=DEMO_KEY\"\n",
    "nasa_response = requests.get(nasa_url).json()\n",
    "\n",
    "print(\"Title:\", nasa_response[\"title\"])\n",
    "print(\"Date:\", nasa_response[\"date\"])\n",
    "print(\"Explanation:\", nasa_response[\"explanation\"][:200], \"...\")\n",
    "\n",
    "df_nasa = pd.DataFrame([{\n",
    "    \"title\": nasa_response[\"title\"],\n",
    "    \"date\": nasa_response[\"date\"],\n",
    "    \"url\": nasa_response[\"url\"]\n",
    "}])\n",
    "display(df_nasa)\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# Part 2: Web Scraping with BeautifulSoup\n",
    "# ------------------------------\n",
    "\n",
    "print(\"\\n=== Example 3: Scraping Quotes Website ===\")\n",
    "\n",
    "url = \"https://quotes.toscrape.com/\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "# Extract quotes and authors\n",
    "quotes = [q.get_text() for q in soup.find_all(\"span\", class_=\"text\")]\n",
    "authors = [a.get_text() for a in soup.find_all(\"small\", class_=\"author\")]\n",
    "tags = [[tag.get_text() for tag in q.find_all(\"a\", class_=\"tag\")] \n",
    "        for q in soup.find_all(\"div\", class_=\"quote\")]\n",
    "\n",
    "df_scrape = pd.DataFrame({\"quote\": quotes, \"author\": authors, \"tags\": tags})\n",
    "display(df_scrape.head())\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# Mission Task\n",
    "# ------------------------------\n",
    "# 1. Use another API from https://github.com/public-apis/public-apis\n",
    "#    (e.g., weather, space, or COVID-19 data).\n",
    "# 2. Scrape multiple pages from https://quotes.toscrape.com/ \n",
    "#    (hint: check \"Next\" button links).\n",
    "# 3. Build a dataset with at least 50 quotes, authors, and tags.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
